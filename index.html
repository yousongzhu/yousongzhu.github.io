<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Yousong Zhu</title>
  
  <meta name="author" content="Yousong Zhu">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Yousong Zhu</name>
              </p>
              <p>
                I am now an Associate Professor at the National Laboratory of Pattern Recognition (NLPR), Institute of Automation Chinese Academic of Sciences (CASIA).
              </p>
                <p>
                I am interested in computer vision and machine learning, especially object detection & recognition, visual self-supervised learning, visual-language models, network architecture design, etc.
              </p>
                <p>
                Looking for interns working on vision foundation model, object detection and visual-language learning. If you are interested, please send me an email with your CV. (课题组常年招收优秀实习生(北京/武汉))
              </p>
              <p style="text-align:center">
                <a href="mailto:yousong.zhu@nlpr.ia.ac.cn">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=l4Oqo8sAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="http://www.ia.cas.cn/sourcedb_ia_cas/cn/iaexpert/202212/t20221219_6585763.html">Chinese homepage</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/yousongzhu.jpg"><img style="width:100%;max-width:100%;border-radius:50%" alt="profile photo" src="images/yousongzhu.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <hr>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
                <td style="padding:10px;width:100%;vertical-align:middle">
                    <heading>News (coming soon)</heading>
                </td>
            </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
            <ul>
              <li>[02/2023] <b><a href="https://arxiv.org/abs/2302.07387">PolyFormer</a> on referring image segmentation was accepted by CVPR 2023!</b></li>
              <li>[01/2023] <b><a href="https://arxiv.org/abs/2208.02131">MaskVLM</a> was accepted by ICLR 2023!</b></li>
              <li>[12/2022] <b>The code of <a href="https://github.com/amazon-science/semi-vit">Semi-ViT</a> has been released!</b></li>
              <li>[10/2022] I will be serving as an Area Chair for CVPR 2023 and ICCV 2023.</li>
              <li>[09/2022] We start hiring research interns for 2023 on various computer vision topics at AWS AI Labs. If you are interested, please drop me an email at zhaoweic@amazon.com!/li>
              <li>[09/2022] <b>Semi-ViT was accepted by NeurIPS 2022. The codes will be coming soon!</b></li>
              <li>[08/2022] Check out our recent works (<a href="https://arxiv.org/abs/2208.05688">semi-supervised ViT</a> and <a href="https://arxiv.org/abs/2208.02131">masked vision and language modeling</a>).</li>
              <li>[07/2022] Two papers (X-DETR and few-shot detection benchmark) were accepted by ECCV 2022. The codes will be coming soon!</li>
              <li>[06/2022] The code of Omni-DETR has been released! Check our <a href="https://github.com/amazon-research/omni-detr">code</a>.</li>
              <li>[03/2022] Omni-DETR was accepted by CVPR 2022. The code is coming soon!</li>
              <li>[06/2021] The code of EMAN has been released! Check our <a href="https://github.com/amazon-research/exponential-moving-average-normalization">code</a>.</li>
              <li>[02/2021] The <a href="https://arxiv.org/pdf/2101.08482.pdf">EMAN</a> paper was accepted by <b>CVPR 2021</b> as <b>Oral</b>.</li>
            </ul>
          </td>
        </tr>
        </tbody></table>
        <hr>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
                <td style="padding:10px;width:100%;vertical-align:middle">
                    <heading>Education</heading>
                </td>
            </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
            <ul>
              <li>Ph.D in Institute of Automation, Chinese Academic of Sciences, 2019.</li>
              <li>B.S. in School of Information Science and Engineering, Central South University, 2014.</li>
            </ul>
          </td>
        </tr>
        </tbody></table>
        <hr>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
                <td style="padding:10px;width:100%;vertical-align:middle">
                    <heading>Work Experience</heading>
                </td>
            </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
            <ul>
              <li><b>2022~now</b>: Associate Professor, National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences.</li>
              <li><b>2019~2022</b>: Assistant Professor, National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences.</li>
            </ul>
          </td>
        </tr>
        </tbody></table>
        <hr>
        
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:10px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>          

        <tr>
            <td style="padding:20px;width:35%;vertical-align:top">
                <img src='images/arXiv2023-EMAE.jpg' width="250"></div>
                    </div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/pdf/2302.14431.pdf">
                    <papertitle>Efficient Masked Autoencoders with Self-Consistency</papertitle>
                </a>
                <br>
                Zhaowen Li, <b>Yousong Zhu</b>, Zhiyang Chen, Wei Li, Chaoyang Zhao, Liwei Wu, Rui Zhao, Ming Tang, Jinqiao Wang
                <br>
                arXiv, 2023
                <br>
                <a href="https://arxiv.org/pdf/2302.14431.pdf">arXiv</a> /
                code
            </td>
        </tr>
    
        <tr>
            <td style="padding:20px;width:35%;vertical-align:top">
                <img src='images/AAAI2023-SAIM.jpg' width="250"></div>
                    </div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://ojs.aaai.org/index.php/AAAI/article/download/25300/25072">
                    <papertitle>Exploring Stochastic Autoregressive Image Modeling for Visual Representation</papertitle>
                </a>
                <br>
                Yu Qi, Fan Yang, <b>Yousong Zhu</b>, Yufei Liu, Liwei Wu, Rui Zhao, Wei Li
                <br>
                AAAI, 2023
                <br>
                <a href="https://ojs.aaai.org/index.php/AAAI/article/download/25300/25072">Paper</a>
            </td>
        </tr>
    
        <tr>
            <td style="padding:20px;width:35%;vertical-align:top">
                <img src='images/NeurIPS2022-Obj2Seq.jpg' width="250"></div>
                    </div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/112bfcff816203efbb986bc1">
                    <papertitle>Obj2Seq: Formatting Objects as Sequences with Class Prompt for Visual Tasks</papertitle>
                </a>
                <br>
                Zhiyang Chen, <b>Yousong Zhu</b>, Zhaowen Li, Fan Yang, Wei Li, Haixin Wang, Chaoyang Zhao, Liwei Wu, Rui Zhao, Jinqiao Wang, Ming Tang
                <br>
                NeurIPS, 2022 (<b>Spotlight</b>)
                <br>
                <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/112bfcff816203efbb986bc1">Paper</a> /
                <a href="https://github.com/CASIA-IVA-Lab/Obj2Seq">code</a> 
            </td>
        </tr>
    
        <tr>
            <td style="padding:20px;width:35%;vertical-align:top">
                <img src='images/ECCV2022-PASS.jpg' width="250"></div>
                    </div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/pdf/2203.03931.pdf">
                    <papertitle>PASS: Part-Aware Self-Supervised Pre-Training for Person Re-Identification</papertitle>
                </a>
                <br>
                Kuan Zhu, Haiyun Guo, Tianyi Yan, <b>Yousong Zhu</b>, Jinqiao Wang, Ming Tang
                <br>
                ECCV, 2022
                <br>
                <a href="https://arxiv.org/pdf/2203.03931.pdf">arXiv</a> /
                <a href="https://github.com/CASIA-IVA-Lab/PASS-reID">code</a>
            </td>
        </tr>
    
        <tr>
            <td style="padding:20px;width:35%;vertical-align:top">
                <img src='images/CVPR2022-UniVIP.jpg' width="250"></div>
                    </div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="http://openaccess.thecvf.com/content/CVPR2022/papers/Li_UniVIP_A_Unified_Framework_for_Self-Supervised_Visual_Pre-Training_CVPR_2022_paper.pdf">
                    <papertitle>UniVIP: A Unified Framework for Self-Supervised Visual Pre-training</papertitle>
                </a>
                <br>
                Zhaowen Li, <b>Yousong Zhu</b>, Fan Yang, Wei Li, Chaoyang Zhao, Yingying Chen, Zhiyang Chen, Jiahao Xie, Liwei Wu, Rui Zhao, Ming Tang, Jinqiao Wang
                <br>
                CVPR, 2022
                <br>
                <a href="http://openaccess.thecvf.com/content/CVPR2022/papers/Li_UniVIP_A_Unified_Framework_for_Self-Supervised_Visual_Pre-Training_CVPR_2022_paper.pdf">Paper</a> 
            </td>
        </tr>
          
        <tr>
            <td style="padding:20px;width:35%;vertical-align:top">
                <img src='images/CVPR2022-C2AM.jpg' width="250"></div>
                    </div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="http://openaccess.thecvf.com/content/CVPR2022/papers/Wang_C2AM_Loss_Chasing_a_Better_Decision_Boundary_for_Long-Tail_Object_CVPR_2022_paper.pdf">
                    <papertitle>C2AM Loss: Chasing a Better Decision Boundary for Long-Tail Object Detection</papertitle>
                </a>
                <br>
                Tong Wang, <b>Yousong Zhu</b>, Yingying Chen, Chaoyang Zhao, Bin Yu, Jinqiao Wang, Ming Tang
                <br>
                CVPR, 2022
                <br>
                <a href="http://openaccess.thecvf.com/content/CVPR2022/papers/Wang_C2AM_Loss_Chasing_a_Better_Decision_Boundary_for_Long-Tail_Object_CVPR_2022_paper.pdf">Paper</a> 
            </td>
        </tr>
    
        <tr>
            <td style="padding:20px;width:35%;vertical-align:top">
                <img src='images/NeurIPS2021-MST.jpg' width="250"></div>
                    </div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://proceedings.neurips.cc/paper_files/paper/2021/file/6dbbe6abe5f14af882ff977fc3f35501-Paper.pdf">
                    <papertitle>MST: Masked Self-Supervised Transformer for Visual Representation</papertitle>
                </a>
                <br>
                Zhaowen Li, Zhiyang Chen, Fan Yang, Wei Li, <b>Yousong Zhu</b>, Chaoyang Zhao, Rui Deng, Liwei Wu, Rui Zhao, Ming Tang, Jinqiao Wang
                <br>
                NeurIPS, 2021
                <br>
                <a href="https://proceedings.neurips.cc/paper_files/paper/2021/file/6dbbe6abe5f14af882ff977fc3f35501-Paper.pdf">Paper</a>
            </td>
        </tr>
    
        <tr>
            <td style="padding:20px;width:35%;vertical-align:top">
                <img src='images/ACM-MM2021-DPT.jpg' width="250"></div>
                    </div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/pdf/2107.14467.pdf">
                    <papertitle>DPT: Deformable Patch-based Transformer for Visual Recognition</papertitle>
                </a>
                <br>
                Zhiyang Chen, <b>Yousong Zhu</b>, Chaoyang Zhao, Guosheng Hu, Wei Zeng, Jinqiao Wang, Ming Tang
                <br>
                ACM MM, 2021 (<b>Oral</b>)
                <br>
                <a href="https://arxiv.org/pdf/2107.14467.pdf">arXiv</a> /
                <a href="https://github.com/CASIA-IVA-Lab/DPT">code</a>
            </td>
        </tr>
    
        <tr>
            <td style="padding:20px;width:35%;vertical-align:top">
                <img src='images/CVPR2021-ACSL.jpg' width="250"></div>
                    </div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_Adaptive_Class_Suppression_Loss_for_Long-Tail_Object_Detection_CVPR_2021_paper.pdf">
                    <papertitle>Adaptive Class Suppression Loss for Long-Tail Object Detection</papertitle>
                </a>
                <br>
                Tong Wang, <b>Yousong Zhu</b>, Chaoyang Zhao, Wei Zeng, Jinqiao Wang, Ming Tang
                <br>
                CVPR, 2021
                <br>
                <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_Adaptive_Class_Suppression_Loss_for_Long-Tail_Object_Detection_CVPR_2021_paper.pdf">Paper</a> /
                <a href="https://github.com/CASIA-IVA-Lab/ACSL">code</a>
            </td>
        </tr>
    
        <tr>
            <td style="padding:20px;width:35%;vertical-align:top">
                <img src='images/ECCV2020-LargeDet.jpg' width="250"></div>
                    </div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123660477.pdf">
                    <papertitle>Large Batch Optimization for Object Detection: Training COCO in 12 Minutes</papertitle>
                </a>
                <br>
                Tong Wang, <b>Yousong Zhu</b>, Chaoyang Zhao, Wei Zeng, Yaowei Wang, Jinqiao Wang, Ming Tang
                <br>
                ECCV, 2020
                <br>
                <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123660477.pdf">Paper</a>
            </td>
        </tr>
    
        <tr>
            <td style="padding:20px;width:35%;vertical-align:top">
                <img src='images/CVPR2020-DSRL.jpg' width="250"></div>
                    </div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Dual_Super-Resolution_Learning_for_Semantic_Segmentation_CVPR_2020_paper.pdf">
                    <papertitle>Dual Super-Resolution Learning for Semantic Segmentation</papertitle>
                </a>
                <br>
                Li Wang, Dong Li, <b>Yousong Zhu</b>, Lu Tian, Yi Shan
                <br>
                CVPR, 2020 (<b>Oral</b>)
                <br>
                <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Dual_Super-Resolution_Learning_for_Semantic_Segmentation_CVPR_2020_paper.pdf">Paper</a> 
            </td>
        </tr>

        <tr>
            <td style="padding:20px;width:35%;vertical-align:top">
                <img src='images/Neurocomputing2020-FoodDet.jpg' width="250"></div>
                    </div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://www.sciencedirect.com/science/article/abs/pii/S0925231219315498">
                    <papertitle>Food det: Detecting Foods in Refrigerator with Supervised Transformer Network</papertitle>
                </a>
                <b>Yousong Zhu*</b>, Xu Zhao*, Chaoyang Zhao, Jinqiao Wang, Hanqing Lu (*equal contribution)
                <br>
                Neurocomputing, 2020
                <br>
                <a href="https://www.sciencedirect.com/science/article/abs/pii/S0925231219315498">Paper</a>
            </td>
        </tr>

     <tr>
            <td style="padding:20px;width:35%;vertical-align:top">
                <img src='images/ICME2019-distillation.jpg' width="250"></div>
                    </div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://ieeexplore.ieee.org/abstract/document/8784933/">
                    <papertitle>Mask Guided Knowledge Distillation for Single Shot Detector</papertitle>
                </a>
                <b>Yousong Zhu</b>, Chaoyang Zhao, Chenxia Han, Jinqiao Wang, Hanqing Lu
                <br>
                ICME, 2019
                <br>
                <a href="https://ieeexplore.ieee.org/abstract/document/8784933/">Paper</a>
            </td>
        </tr>

        <tr>
            <td style="padding:20px;width:35%;vertical-align:top">
                <img src='images/CoupleNet.jpg' width="250"></div>
                    </div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8434341">
                    <papertitle>Attention CoupleNet: Fully Convolutional Attention Coupling Network for Object Detection</papertitle>
                </a>
                <b>Yousong Zhu</b>, Chaoyang Zhao, Haiyun Guo, Jinqiao Wang, Xu Zhao, Hanqing Lu
                <br>
                TIP, 2019
                <br>
                <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8434341">Paper</a> 
                  
                <p>
                <a href="https://openaccess.thecvf.com/content_ICCV_2017/papers/Zhu_CoupleNet_Coupling_Global_ICCV_2017_paper.pdf">
                    <papertitle>CoupleNet: Coupling Global Structure with Local Parts for Object Detection</papertitle>
                </a>
                <b>Yousong Zhu</b>, Chaoyang Zhao, Jinqiao Wang, Xu Zhao, Yi Wu, Hanqing Lu
                <br>
                ICCV, 2017
                <br>
                <a href="https://openaccess.thecvf.com/content_ICCV_2017/papers/Zhu_CoupleNet_Coupling_Global_ICCV_2017_paper.pdf">Paper</a> /
                <a href="https://github.com/yousongzhu/CoupleNet">code</a>
            </td>
        </tr>  

        <tr>
            <td style="padding:20px;width:35%;vertical-align:top">
                <img src='images/ACCV2016-SADR.jpg' width="250"></div>
                    </div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://www.researchgate.net/profile/Yousong-Zhu/publication/314522181_Scale-Adaptive_Deconvolutional_Regression_Network_for_Pedestrian_Detection/links/5b278e8f458515cad55f82c8/Scale-Adaptive-Deconvolutional-Regression-Network-for-Pedestrian-Detection.pdf">
                    <papertitle>Scale-Adaptive Deconvolutional Regression Network for Pedestrian Detection</papertitle>
                </a>
                <b>Yousong Zhu</b>, Jinqiao Wang, Chaoyang Zhao, Haiyun Guo, Hanqing Lu
                <br>
                ACCV, 2016
                <br>
                <a href="https://www.researchgate.net/profile/Yousong-Zhu/publication/314522181_Scale-Adaptive_Deconvolutional_Regression_Network_for_Pedestrian_Detection/links/5b278e8f458515cad55f82c8/Scale-Adaptive-Deconvolutional-Regression-Network-for-Pedestrian-Detection.pdf">Paper</a>
            </td>
        </tr>   


        </tbody></table>
        <hr>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
                <td style="padding:10px;width:100%;vertical-align:middle">
                    <heading>Service</heading>
                </td>
            </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
            <ul>
              <li>Area Chair: ICCV23, CVPR23</li>
              <li>Workshop co-organizer of <a href="https://eccv20-adv-workshop.github.io/">Adversarial Robustness in the Real World</a> in ECCV 2020.</li>
              <li>Journal Reviewer: T-PAMI, IJCV, T-IP, CVIU, T-MM, PR, T-CSVT, T-ITS, T-Cybernetics</li>
              <li>Conference Reviewer: NeurIPS22, ICML22, CVPR22, ICLR22, NeurIPS21, ICCV21, ICML21, CVPR21, ICLR21, NeurIPS20, ICML20, ECCV20, CVPR20, NeurIPS19, ICML19, ICCV19, CVPR19 (outstanding reviewer), NIPS18, ECCV18, ICML18, CVPR18, NIPS17, ICCV17, CVPR17</li>
            </ul>
          </td>
        </tr>
        </tbody></table>
        <hr>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
                <td style="padding:10px;width:100%;vertical-align:middle">
                    <heading>Talks</heading>
                </td>
            </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
            <ul>
              <li>Invited talk at UCSC AI seminar: Image/Object/Mask-level Vision and Language Understanding.</li>
              <li>Invited talk at UCSB NLP group: Pushing the Limits of Object Detection.</li>
              <li>Invited talk at UCSB: Low-precision Neural Networks.</li>
              <li>Guest lecture at UCSC: Exponential Moving Average Normalization for Self- and Semi- Supervised Learning.</li>
            </ul>
          </td>
        </tr>
        </tbody></table>
        <hr>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">template credit to <a href="https://jonbarron.info/">Jon Barron</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
