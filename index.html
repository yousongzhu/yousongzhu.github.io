<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Yousong Zhu</title>
  
  <meta name="author" content="Yousong Zhu">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Yousong Zhu</name>
              </p>
              <p style="font-size: 16px">
                I am now an Associate Professor at the National Laboratory of Pattern Recognition (NLPR), Institute of Automation, Chinese Academy of Sciences (CASIA).
              </p>
                <p style="font-size: 16px">
                I am interested in computer vision and machine learning, especially object detection & recognition, visual self-supervised learning, visual-language models, network architecture design, etc.
              </p>
                <p style="font-size: 16px">
                Looking for interns working on vision foundation model, object detection and visual-language learning. If you are interested, please send me an email with your CV.
              </p>
              <p style="text-align:center;font-size: 16px">
                <a href="mailto:yousong.zhu@nlpr.ia.ac.cn">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=l4Oqo8sAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="http://www.ia.cas.cn/sourcedb_ia_cas/cn/iaexpert/202212/t20221219_6585763.html">Chinese homepage</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/yousongzhu.jpg"><img style="width:100%;max-width:100%;border-radius:50%" alt="profile photo" src="images/yousongzhu.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <hr>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
                <td style="padding:10px;width:100%;vertical-align:middle">
                    <heading>News! </heading>
                </td>
            </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
            <ul>
              <li>[02/2023] Check out our recent work (arXiv: <a href="https://arxiv.org/pdf/2302.14431.pdf">EMAE</a>) </li>
              <li>[11/2022] One paper working on <b>Autoregressive Image Modeling</b> was accepted by AAAI2023.</li>
              <li>[09/2022] The paper <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/112bfcff816203efbb986bc178380ef2-Paper-Conference.pdf">Obj2Seq</a> was accepted by <b>NeurIPS2022</b> as <b>Spotlight</b>! Check our <a href="https://github.com/CASIA-IVA-Lab/Obj2Seq">code</a>.</li>
              <li>[07/2022] The paper <a href="https://arxiv.org/pdf/2203.03931.pdf">PASS</a> working on Person Re-identification was accepted by ECCV2022, the code has been <a href="https://github.com/CASIA-IVA-Lab/PASS-reID">released</a>!</li>
              <li>[03/2022] Two papers (UniVIP and C2AM loss for long-tail detection) were accepted by CVPR2022.</li>
              <li>[09/2021] We tried <b>Masked Image Modeling</b> in visual representation learning, the paper <a href="https://proceedings.neurips.cc/paper_files/paper/2021/file/6dbbe6abe5f14af882ff977fc3f35501-Paper.pdf">MST</a> was accepted by NeurIPS2021.</li>
              <li>[07/2021] Our paper <a href="https://arxiv.org/pdf/2107.14467.pdf">DPT</a> working on Deformable Transformer was accepted by <b>ACM MM2021</b> as <b>Oral</b>, Check our <a href="https://github.com/CASIA-IVA-Lab/DPT">code</a>.</li>
              <li>[03/2021] The paper <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_Adaptive_Class_Suppression_Loss_for_Long-Tail_Object_Detection_CVPR_2021_paper.pdf">ACSL</a> was accepted by CVPR2021, the code has been <a href="https://github.com/CASIA-IVA-Lab/ACSL">released</a>!</li>
              <li>[07/2020] the paper working on <b>Large Batch Training</b> for object detection was accepted by ECCV2020. Training a Faster R-CNN in COCO within <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123660477.pdf">12 minutes</a>.</li>
              <li>The <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Dual_Super-Resolution_Learning_for_Semantic_Segmentation_CVPR_2020_paper.pdf">DSRL</a> paper was accepted by CVPR2020 as Oral, Congrats to all collaborators!</li>
            </ul>
          </td>
        </tr>
        </tbody></table>
        <hr>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
                <td style="padding:10px;width:100%;vertical-align:middle">
                    <heading>Education</heading>
                </td>
            </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
            <ul>
              <li style="font-size: 16px">Ph.D in Institute of Automation, Chinese Academy of Sciences, 2019.</li>
              <li style="font-size: 16px">B.S. in School of Information Science and Engineering, Central South University, 2014.</li>
            </ul>
          </td>
        </tr>
        </tbody></table>
        <hr>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
                <td style="padding:10px;width:100%;vertical-align:middle">
                    <heading>Work Experience</heading>
                </td>
            </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
            <ul>
              <li style="font-size: 16px"><b>2022~Now</b>: Associate Professor
                <ul>
                  <li style="font-size: 14px">National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences.</li>
                </ul>
              </li>
              <li style="font-size: 16px"><b>2019~2021</b>: Assistant Professor
                <ul>
                  <li style="font-size: 14px">National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences.</li>
                </ul>
              </li>
            </ul>
          </td>
        </tr>
        </tbody></table>
        <hr>
        
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:10px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>          

        <tr>
            <td style="padding:20px;width:35%;vertical-align:top">
                <img src='images/arXiv2023-EMAE.jpg' width="250"></div>
                    </div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/pdf/2302.14431.pdf">
                    <papertitle>Efficient Masked Autoencoders with Self-Consistency</papertitle>
                </a>
                <br>
                Zhaowen Li, <b>Yousong Zhu</b>, Zhiyang Chen, Wei Li, Chaoyang Zhao, Liwei Wu, Rui Zhao, Ming Tang, Jinqiao Wang
                <br>
                arXiv, 2023
                <br>
                <a href="https://arxiv.org/pdf/2302.14431.pdf">arXiv</a> /
                code
            </td>
        </tr>
    
        <tr>
            <td style="padding:20px;width:35%;vertical-align:top">
                <img src='images/AAAI2023-SAIM.jpg' width="250"></div>
                    </div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://ojs.aaai.org/index.php/AAAI/article/download/25300/25072">
                    <papertitle>Exploring Stochastic Autoregressive Image Modeling for Visual Representation</papertitle>
                </a>
                <br>
                Yu Qi, Fan Yang, <b>Yousong Zhu</b>, Yufei Liu, Liwei Wu, Rui Zhao, Wei Li
                <br>
                Thirty-Seventh AAAI Conference on Artificial Intelligence (<b>AAAI</b>), 2023
                <br>
                <a href="https://ojs.aaai.org/index.php/AAAI/article/download/25300/25072">Paper</a>
            </td>
        </tr>
    
        <tr>
            <td style="padding:20px;width:35%;vertical-align:top">
                <img src='images/NeurIPS2022-Obj2Seq.jpg' width="250"></div>
                    </div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/112bfcff816203efbb986bc1">
                    <papertitle>Obj2Seq: Formatting Objects as Sequences with Class Prompt for Visual Tasks</papertitle>
                </a>
                <br>
                Zhiyang Chen, <b>Yousong Zhu</b>, Zhaowen Li, Fan Yang, Wei Li, Haixin Wang, Chaoyang Zhao, Liwei Wu, Rui Zhao, Jinqiao Wang, Ming Tang
                <br>
                Neural Information Processing Systems (<b>NeurIPS</b>), 2022 (<b>Spotlight</b>)
                <br>
                <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/112bfcff816203efbb986bc1">Paper</a> /
                <a href="https://github.com/CASIA-IVA-Lab/Obj2Seq">code</a> 
            </td>
        </tr>
    
        <tr>
            <td style="padding:20px;width:35%;vertical-align:top">
                <img src='images/ECCV2022-PASS.jpg' width="250"></div>
                    </div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/pdf/2203.03931.pdf">
                    <papertitle>PASS: Part-Aware Self-Supervised Pre-Training for Person Re-Identification</papertitle>
                </a>
                <br>
                Kuan Zhu, Haiyun Guo, Tianyi Yan, <b>Yousong Zhu</b>, Jinqiao Wang, Ming Tang
                <br>
                European Conference on Computer Vision (<b>ECCV</b>), 2022
                <br>
                <a href="https://arxiv.org/pdf/2203.03931.pdf">arXiv</a> /
                <a href="https://github.com/CASIA-IVA-Lab/PASS-reID">code</a>
            </td>
        </tr>
    
        <tr>
            <td style="padding:20px;width:35%;vertical-align:top">
                <img src='images/CVPR2022-UniVIP.jpg' width="250"></div>
                    </div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="http://openaccess.thecvf.com/content/CVPR2022/papers/Li_UniVIP_A_Unified_Framework_for_Self-Supervised_Visual_Pre-Training_CVPR_2022_paper.pdf">
                    <papertitle>UniVIP: A Unified Framework for Self-Supervised Visual Pre-training</papertitle>
                </a>
                <br>
                Zhaowen Li, <b>Yousong Zhu</b>, Fan Yang, Wei Li, Chaoyang Zhao, Yingying Chen, Zhiyang Chen, Jiahao Xie, Liwei Wu, Rui Zhao, Ming Tang, Jinqiao Wang
                <br>
                IEEE Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2022
                <br>
                <a href="http://openaccess.thecvf.com/content/CVPR2022/papers/Li_UniVIP_A_Unified_Framework_for_Self-Supervised_Visual_Pre-Training_CVPR_2022_paper.pdf">Paper</a> 
            </td>
        </tr>
          
        <tr>
            <td style="padding:20px;width:35%;vertical-align:top">
                <img src='images/CVPR2022-C2AM.jpg' width="250"></div>
                    </div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="http://openaccess.thecvf.com/content/CVPR2022/papers/Wang_C2AM_Loss_Chasing_a_Better_Decision_Boundary_for_Long-Tail_Object_CVPR_2022_paper.pdf">
                    <papertitle>C2AM Loss: Chasing a Better Decision Boundary for Long-Tail Object Detection</papertitle>
                </a>
                <br>
                Tong Wang, <b>Yousong Zhu</b>, Yingying Chen, Chaoyang Zhao, Bin Yu, Jinqiao Wang, Ming Tang
                <br>
                IEEE Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2022
                <br>
                <a href="http://openaccess.thecvf.com/content/CVPR2022/papers/Wang_C2AM_Loss_Chasing_a_Better_Decision_Boundary_for_Long-Tail_Object_CVPR_2022_paper.pdf">Paper</a> 
            </td>
        </tr>
    
        <tr>
            <td style="padding:20px;width:35%;vertical-align:top">
                <img src='images/NeurIPS2021-MST.jpg' width="250"></div>
                    </div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://proceedings.neurips.cc/paper_files/paper/2021/file/6dbbe6abe5f14af882ff977fc3f35501-Paper.pdf">
                    <papertitle>MST: Masked Self-Supervised Transformer for Visual Representation</papertitle>
                </a>
                <br>
                Zhaowen Li, Zhiyang Chen, Fan Yang, Wei Li, <b>Yousong Zhu</b>, Chaoyang Zhao, Rui Deng, Liwei Wu, Rui Zhao, Ming Tang, Jinqiao Wang
                <br>
                Neural Information Processing Systems (<b>NeurIPS</b>), 2021
                <br>
                <a href="https://proceedings.neurips.cc/paper_files/paper/2021/file/6dbbe6abe5f14af882ff977fc3f35501-Paper.pdf">Paper</a>
            </td>
        </tr>
    
        <tr>
            <td style="padding:20px;width:35%;vertical-align:top">
                <img src='images/ACM-MM2021-DPT.jpg' width="250"></div>
                    </div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://arxiv.org/pdf/2107.14467.pdf">
                    <papertitle>DPT: Deformable Patch-based Transformer for Visual Recognition</papertitle>
                </a>
                <br>
                Zhiyang Chen, <b>Yousong Zhu</b>, Chaoyang Zhao, Guosheng Hu, Wei Zeng, Jinqiao Wang, Ming Tang
                <br>
                ACM International Conference on Multimedia (<b>ACM MM</b>), 2021 (<b>Oral</b>)
                <br>
                <a href="https://arxiv.org/pdf/2107.14467.pdf">arXiv</a> /
                <a href="https://github.com/CASIA-IVA-Lab/DPT">code</a>
            </td>
        </tr>
    
        <tr>
            <td style="padding:20px;width:35%;vertical-align:top">
                <img src='images/CVPR2021-ACSL.jpg' width="250"></div>
                    </div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_Adaptive_Class_Suppression_Loss_for_Long-Tail_Object_Detection_CVPR_2021_paper.pdf">
                    <papertitle>Adaptive Class Suppression Loss for Long-Tail Object Detection</papertitle>
                </a>
                <br>
                Tong Wang, <b>Yousong Zhu</b>, Chaoyang Zhao, Wei Zeng, Jinqiao Wang, Ming Tang
                <br>
                IEEE Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2021
                <br>
                <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_Adaptive_Class_Suppression_Loss_for_Long-Tail_Object_Detection_CVPR_2021_paper.pdf">Paper</a> /
                <a href="https://github.com/CASIA-IVA-Lab/ACSL">code</a>
            </td>
        </tr>
    
        <tr>
            <td style="padding:20px;width:35%;vertical-align:top">
                <img src='images/ECCV2020-LargeDet.jpg' width="250"></div>
                    </div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123660477.pdf">
                    <papertitle>Large Batch Optimization for Object Detection: Training COCO in 12 Minutes</papertitle>
                </a>
                <br>
                Tong Wang, <b>Yousong Zhu</b>, Chaoyang Zhao, Wei Zeng, Yaowei Wang, Jinqiao Wang, Ming Tang
                <br>
                European Conference on Computer Vision (<b>ECCV</b>), 2020
                <br>
                <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123660477.pdf">Paper</a>
            </td>
        </tr>
    
        <tr>
            <td style="padding:20px;width:35%;vertical-align:top">
                <img src='images/CVPR2020-DSRL.jpg' width="250"></div>
                    </div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Dual_Super-Resolution_Learning_for_Semantic_Segmentation_CVPR_2020_paper.pdf">
                    <papertitle>Dual Super-Resolution Learning for Semantic Segmentation</papertitle>
                </a>
                <br>
                Li Wang, Dong Li, <b>Yousong Zhu</b>, Lu Tian, Yi Shan
                <br>
                IEEE Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2020 (<b>Oral</b>)
                <br>
                <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Dual_Super-Resolution_Learning_for_Semantic_Segmentation_CVPR_2020_paper.pdf">Paper</a> 
            </td>
        </tr>

        <tr>
            <td style="padding:20px;width:35%;vertical-align:top">
                <img src='images/Neurocomputing2020-FoodDet.jpg' width="250"></div>
                    </div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://www.sciencedirect.com/science/article/abs/pii/S0925231219315498">
                    <papertitle>Food det: Detecting Foods in Refrigerator with Supervised Transformer Network</papertitle>
                </a>
                <br>
                <b>Yousong Zhu*</b>, Xu Zhao*, Chaoyang Zhao, Jinqiao Wang, Hanqing Lu (*equal contribution)
                <br>
                Neurocomputing, 2020
                <br>
                <a href="https://www.sciencedirect.com/science/article/abs/pii/S0925231219315498">Paper</a>
            </td>
        </tr>

     <tr>
            <td style="padding:20px;width:35%;vertical-align:top">
                <img src='images/ICME2019-distillation.jpg' width="250"></div>
                    </div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://ieeexplore.ieee.org/abstract/document/8784933/">
                    <papertitle>Mask Guided Knowledge Distillation for Single Shot Detector</papertitle>
                </a>
                <br>
                <b>Yousong Zhu</b>, Chaoyang Zhao, Chenxia Han, Jinqiao Wang, Hanqing Lu
                <br>
                IEEE International Conference on Multimedia & Expo (ICME), 2019
                <br>
                <a href="https://ieeexplore.ieee.org/abstract/document/8784933/">Paper</a>
            </td>
        </tr>

        <tr>
            <td style="padding:20px;width:35%;vertical-align:top">
                <img src='images/CoupleNet.jpg' width="250"></div>
                    </div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8434341">
                    <papertitle>Attention CoupleNet: Fully Convolutional Attention Coupling Network for Object Detection</papertitle>
                </a>
                <br>
                <b>Yousong Zhu</b>, Chaoyang Zhao, Haiyun Guo, Jinqiao Wang, Xu Zhao, Hanqing Lu
                <br>
                IEEE Transactions on Image Processing (<b>TIP</b>), 2019
                <br>
                <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8434341">Paper</a> 
                  
                <p>
                <a href="https://openaccess.thecvf.com/content_ICCV_2017/papers/Zhu_CoupleNet_Coupling_Global_ICCV_2017_paper.pdf">
                    <papertitle>CoupleNet: Coupling Global Structure with Local Parts for Object Detection</papertitle>
                </a>
                <br>
                <b>Yousong Zhu</b>, Chaoyang Zhao, Jinqiao Wang, Xu Zhao, Yi Wu, Hanqing Lu
                <br>
                International Conference on Computer Vision (<b>ICCV</b>), 2017
                <br>
                <a href="https://openaccess.thecvf.com/content_ICCV_2017/papers/Zhu_CoupleNet_Coupling_Global_ICCV_2017_paper.pdf">Paper</a> /
                <a href="https://github.com/yousongzhu/CoupleNet">code</a>
            </td>
        </tr>  

        <tr>
            <td style="padding:20px;width:35%;vertical-align:top">
                <img src='images/ACCV2016-SADR.jpg' width="250"></div>
                    </div>
                    </td>
            <td width="75%" valign="middle">
                <p>
                <a href="https://www.researchgate.net/profile/Yousong-Zhu/publication/314522181_Scale-Adaptive_Deconvolutional_Regression_Network_for_Pedestrian_Detection/links/5b278e8f458515cad55f82c8/Scale-Adaptive-Deconvolutional-Regression-Network-for-Pedestrian-Detection.pdf">
                    <papertitle>Scale-Adaptive Deconvolutional Regression Network for Pedestrian Detection</papertitle>
                </a>
                <br>
                <b>Yousong Zhu</b>, Jinqiao Wang, Chaoyang Zhao, Haiyun Guo, Hanqing Lu
                <br>
                Asian Conference on Computer Vision (ACCV), 2016
                <br>
                <a href="https://www.researchgate.net/profile/Yousong-Zhu/publication/314522181_Scale-Adaptive_Deconvolutional_Regression_Network_for_Pedestrian_Detection/links/5b278e8f458515cad55f82c8/Scale-Adaptive-Deconvolutional-Regression-Network-for-Pedestrian-Detection.pdf">Paper</a>
            </td>
        </tr>   


        </tbody></table>
        <hr>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
                <td style="padding:10px;width:100%;vertical-align:middle">
                    <heading>Honors & Service</heading>
                </td>
            </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
            <ul>
              <li>2022 Super AI Leader -World AI Conference (WAIC), SAIL Award. (2022世界人工智能大会最高奖--SAIL奖)</li>
              <li>2022 Gold medal of the 8th China College Students' 'Internet+' Innovation and Entrepreneurship Competition.</li>
              <li>2019 University of Chinese Academy of Sciences & Beijing Outstanding Graduate. (中国科学院大学&北京市优秀毕业生)</li>
              <li>2018 AI Challenger Autonomous Driving Perception, First Place Prize. (AI Challenger全球无人驾驶视觉感知第一名)</li>
              <li>Journal Reviewer: T-IP, T-MM, PR, T-CSVT, T-NNLS, Neurocomputing.</li>
              <li>Conference Reviewer: CVPR23, NeurIPS23, ICCV23, ICLR23, AAAI23, ECCV22, ACM MM22, NeurIPS22, AAAI22, CVPR22, ICCV21, ACM MM21, CVPR21, AAAI21, CVPR20, ECCV20, AAAI20, ICCV19, ACM MM19.</li>
            </ul>
          </td>
        </tr>
        </tbody></table>
        <hr>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
                <td style="padding:10px;width:100%;vertical-align:middle">
                    <heading>Students</heading>
                </td>
            </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
            <ul>
              <li><b>Ph.D</b>:
              <ul>
                Tong Wang (2017-2022; Baidu), work on object detection. (together with Prof. Ming Tang)
                <br>
                Zhiyang Chen (2019- ), work on vision foundation model. (together with Prof. Ming Tang)
                <br>
                Zhaowen Li (2019- ), work on visual self-supervised learning. (together with Prof. Jinqiao Wang)
                <br>
                YuFei Zhan (2021- ), work on open-world object detection and recognition. (together with Prof. Jinqiao Wang)
                <br>
                Fan Yang (2022- ), work on visual prompt learning and language-guided object detection. (together with Prof. Jinqiao Wang)
              </ul>
              </li>
              <li><b>Master</b>:
              <ul>
                Hongyin Zhao (2020-2023), work on long-tail object detection. (together with Prof. Jinqiao Wang)
              </ul>
              </li>
            </ul>
          </td>
        </tr>
        </tbody></table>
        <hr>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">template credit to <a href="https://jonbarron.info/">Jon Barron</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
